"""
Spell check English text in HTML output files under docs/

Usage:
    python ./check_spelling_in_quirkrecs.py
"""

import json
import re
from html.parser import HTMLParser
from pathlib import Path
from spellchecker import SpellChecker


def load_custom_dictionary(
    dict_path: Path,
) -> tuple[set[str], set[str], list[str]]:
    """Load custom words and phrases from JSON dictionary file.

    Returns (words_ci, words_exact, phrases) where:
    - words_ci: case-insensitive words (stored lowercased)
    - words_exact: case-sensitive words (stored in original case,
      with curly apostrophes normalized to straight)
    - phrases: multi-word entries
    """
    words_ci = set()
    words_exact = set()
    phrases = []
    if dict_path.exists():
        data = json.loads(dict_path.read_text(encoding="utf-8"))
        for word in data.get("words", []):
            # Normalize curly apostrophes to straight for lookup
            words_ci.add(word.replace("\u2019", "'").lower())
        for word in data.get("words_exact", []):
            # Normalize curly apostrophes but preserve case
            words_exact.add(word.replace("\u2019", "'"))
        phrases = data.get("phrases", [])
    return words_ci, words_exact, phrases


def extract_english_words(text: str) -> list[str]:
    """Extract English words from text, ignoring Hebrew and sigla."""
    # Remove Hebrew characters (includes base letters, vowel points, accents, marks)
    text = re.sub(r"[\u0590-\u05FF]+", " ", text)
    # Remove $-prefixed sigla ($BHQ, $yod, $BHL_A, etc.)
    text = re.sub(r"\$[A-Za-z_]+", " ", text)
    # Extract words (letters including scholarly transliteration chars like š, ṣ, ḥ)
    # Include curly apostrophe (\u2019) as part of contractions (e.g. doesn\u2019t)
    # Include μ for sigla like μL, μA
    words = re.findall(r"[a-zA-ZšṣḥŠṢḤμ]+(?:\u2019[a-zA-Z]+)*", text)
    return words


class _TextExtractor(HTMLParser):
    """Extract visible text from HTML, skipping <style>, <script>, and Hebrew-only elements."""

    _SKIP_TAGS = {"style", "script"}

    def __init__(self):
        super().__init__()
        self._pieces: list[str] = []
        self._skip_depth = 0

    def handle_starttag(self, tag, attrs):
        if tag in self._SKIP_TAGS:
            self._skip_depth += 1

    def handle_endtag(self, tag):
        if tag in self._SKIP_TAGS and self._skip_depth > 0:
            self._skip_depth -= 1

    def handle_data(self, data):
        if self._skip_depth == 0:
            self._pieces.append(data)

    def get_text(self) -> str:
        return " ".join(self._pieces)


def _extract_text_from_html(html_path: Path) -> str:
    """Parse an HTML file and return its visible text content."""
    html = html_path.read_text(encoding="utf-8")
    extractor = _TextExtractor()
    extractor.feed(html)
    return extractor.get_text()


def _collect_html_files(docs_dir: Path) -> list[Path]:
    """Collect all .html files under docs/, recursively."""
    return sorted(docs_dir.rglob("*.html"))


def check_straight_apostrophes(html_files: list[Path]):
    """Check for straight apostrophes (U+0027) in HTML text; curly (\u2019) should be used."""
    issues = []
    for html_path in html_files:
        text = _extract_text_from_html(html_path)
        rel = html_path.as_posix()
        for match in re.finditer(r"'", text):
            context = text[max(0, match.start() - 10) : match.end() + 10]
            issues.append({"file": rel, "context": context})
    return issues


def check_period_uppercase(html_files: list[Path]):
    """Check for period immediately followed by an uppercase letter (missing space)."""
    issues = []
    for html_path in html_files:
        text = _extract_text_from_html(html_path)
        rel = html_path.as_posix()
        for match in re.finditer(r"\.[A-Z]", text):
            context = text[max(0, match.start() - 10) : match.end() + 10]
            issues.append({"file": rel, "context": context})
    return issues


def check_spelling(html_files: list[Path], custom_dict_path: Path):
    """Check spelling of English words in HTML files.

    Returns (issues, word_ci_freq, word_exact_freq, phrase_freq) where the
    freq dicts map each custom-dictionary entry to the number of times it
    was matched.
    """
    spell = SpellChecker()
    words_ci, words_exact, custom_phrases = load_custom_dictionary(custom_dict_path)

    # Track how many times each custom entry is matched
    word_ci_freq: dict[str, int] = {w: 0 for w in words_ci}
    word_exact_freq: dict[str, int] = {w: 0 for w in words_exact}
    phrase_freq: dict[str, int] = {p: 0 for p in custom_phrases}

    issues = []

    for html_path in html_files:
        text = _extract_text_from_html(html_path)

        # Normalize whitespace so phrase matching works
        # (the HTML text extractor can produce runs of whitespace)
        cleaned = re.sub(r"\s+", " ", text)

        # Remove accepted phrases before word extraction, counting hits
        for phrase in custom_phrases:
            pattern = re.compile(re.escape(phrase), flags=re.IGNORECASE)
            matches = pattern.findall(cleaned)
            phrase_freq[phrase] += len(matches)
            cleaned = pattern.sub(" ", cleaned)

        words = extract_english_words(cleaned)
        for word in words:
            # Normalize curly apostrophe to straight for lookups
            normalized = word.replace("\u2019", "'")
            lookup_lower = normalized.lower()
            if normalized in words_exact:
                word_exact_freq[normalized] += 1
            elif lookup_lower in words_ci:
                word_ci_freq[lookup_lower] += 1
            elif lookup_lower not in spell:
                rel = html_path.as_posix()
                issues.append(
                    {
                        "file": rel,
                        "word": word,
                        "suggestions": list(
                            spell.candidates(word.lower()) or []
                        )[:5],
                    }
                )

    return issues, word_ci_freq, word_exact_freq, phrase_freq


def main():
    project_root = Path(__file__).parent
    docs_dir = project_root / "docs"
    custom_dict_path = (
        Path(__file__).parent / "check_spelling_in_html.custom-dict.json"
    )

    if not docs_dir.exists():
        print(f"Error: {docs_dir} not found")
        print("Run: python main_gen_misc_authored_english_documents.py")
        return

    html_files = _collect_html_files(docs_dir)
    if not html_files:
        print(f"Error: no HTML files found under {docs_dir}")
        return

    print(f"Checking spelling in {len(html_files)} HTML file(s) under {docs_dir}...")
    print(f"Using custom dictionary: {custom_dict_path}")

    apos_issues = check_straight_apostrophes(html_files)
    if apos_issues:
        print(
            f"\nFound {len(apos_issues)} straight-apostrophe issues (use \u2019 not '):\n"
        )
        for issue in apos_issues:
            print(f"  [{issue['file']}]: ...{issue['context']}...")

    period_issues = check_period_uppercase(html_files)
    if period_issues:
        print(
            f"\nFound {len(period_issues)} period-uppercase issues (missing space?):\n"
        )
        for issue in period_issues:
            print(f"  [{issue['file']}]: ...{issue['context']}...")

    issues, word_ci_freq, word_exact_freq, phrase_freq = check_spelling(
        html_files, custom_dict_path
    )

    # Write custom dictionary frequency reports
    out_dir = project_root / "out"
    out_dir.mkdir(exist_ok=True)

    def _make_freq_report(sorter):
        return {
            "words": {k: v for k, v in sorter(word_ci_freq.items())},
            "words_exact": {k: v for k, v in sorter(word_exact_freq.items())},
            "phrases": {k: v for k, v in sorter(phrase_freq.items())},
        }

    # Alphabetical order
    alpha_path = out_dir / "custom-dict-freqs-ordered-by-entry.json"
    alpha_path.write_text(
        json.dumps(
            _make_freq_report(sorted), indent=4, ensure_ascii=False
        )
        + "\n",
        encoding="utf-8",
    )

    # By count (descending), then alphabetical within same count
    by_count = lambda items: sorted(items, key=lambda x: (-x[1], x[0]))
    count_path = out_dir / "custom-dict-freqs-ordered-by-count.json"
    count_path.write_text(
        json.dumps(
            _make_freq_report(by_count), indent=4, ensure_ascii=False
        )
        + "\n",
        encoding="utf-8",
    )

    print(f"\nCustom dictionary frequencies written to {alpha_path}")
    print(f"Custom dictionary frequencies written to {count_path}")

    # Suggest removals for zero-count entries
    unused_words = sorted(w for w, c in word_ci_freq.items() if c == 0)
    unused_words += sorted(w for w, c in word_exact_freq.items() if c == 0)
    unused_phrases = sorted(p for p, c in phrase_freq.items() if c == 0)
    if unused_words or unused_phrases:
        print("\nSuggested removals from custom dictionary (zero occurrences):")
        for w in unused_words:
            print(f"  word: {w}")
        for p in unused_phrases:
            print(f"  phrase: {p}")
    else:
        print("\nAll custom dictionary entries are in use.")

    if issues:
        print(f"\nFound {len(issues)} potential spelling issues:\n")
        for issue in issues:
            print(f"  [{issue['file']}]: '{issue['word']}'")
            if issue["suggestions"]:
                print(f"    Suggestions: {', '.join(issue['suggestions'])}")

        # Group by word
        word_counts = {}
        for issue in issues:
            word = issue["word"].lower()
            word_counts[word] = word_counts.get(word, 0) + 1

        print("\n--- Summary by word ---")
        for word, count in sorted(word_counts.items(), key=lambda x: -x[1]):
            print(f"  {word}: {count} occurrence(s)")
    else:
        print("\nNo spelling issues found!")

    if apos_issues or period_issues or issues:
        exit(1)


if __name__ == "__main__":
    main()
